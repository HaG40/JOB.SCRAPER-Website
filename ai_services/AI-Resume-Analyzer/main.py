# main.py
# activate && conda activate base
# uvicorn main:app --reload --port 5000

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from groq import Groq
from dotenv import load_dotenv
from PIL import Image
from pdf2image import convert_from_bytes
import pytesseract
import os
import re
from io import BytesIO
from pydantic import BaseModel
import base64

pytesseract.pytesseract.tesseract_cmd = r"C:/Program Files/Tesseract-OCR/tesseract.exe"

load_dotenv()
API_KEY = os.getenv("GROQ_API_KEY")

app = FastAPI(title="AI Resume Analyzer API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

client = Groq(api_key=API_KEY)

conversation_history = []
resume_uploaded = False
resume_cache = ""
interview_history = []

def extract_text(uploaded_file) -> str:
    """Extract text from PDF or Image using OCR"""
    try:
        filename = uploaded_file.filename.lower()
        uploaded_file.file.seek(0)
        content = uploaded_file.file.read()

        if filename.endswith(".pdf"):
            images = convert_from_bytes(content, poppler_path=r"C:/Program Files/poppler-25.07.0/Library/bin")
            text = ""
            for img in images:
                text += pytesseract.image_to_string(img, lang="eng+tha") + "\n"
            return text.strip()

        elif filename.endswith((".png", ".jpg", ".jpeg")):
            image = Image.open(BytesIO(content))
            text = pytesseract.image_to_string(image, lang="eng+tha")
            return text.strip()

        else:
            return "âŒ Unsupported file format"

    except Exception as e:
        return f"âŒ Error extracting text: {str(e)}"

def extract_scores(text: str):
    pattern = r'(\d+(?:\.\d+)?)%'
    matches = re.findall(pattern, text)
    return [float(m) for m in matches]

def file_to_base64(file: UploadFile) -> str:
    """
    à¹à¸›à¸¥à¸‡ UploadFile à¹€à¸›à¹‡à¸™ Base64
    - PDF (à¸«à¸™à¹‰à¸²à¹€à¸”à¸µà¸¢à¸§) â†’ JPG â†’ Base64 string
    - JPG/PNG â†’ Base64 string
    """
    content_type = file.content_type
    file_bytes = file.file.read()
    file.file.close()

    if content_type == "application/pdf":
        images = convert_from_bytes(
            file_bytes,
            poppler_path=r"C:/Program Files/poppler-25.07.0/Library/bin"
        )
        img = images[0]
        buffer = BytesIO()
        img.save(buffer, format="JPEG")
        base64_str = base64.b64encode(buffer.getvalue()).decode("utf-8")
        return base64_str

    elif content_type in ["image/jpeg", "image/jpg", "image/png"]:
        return base64.b64encode(file_bytes).decode("utf-8")
    else:
        raise ValueError("à¹„à¸Ÿà¸¥à¹Œà¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™ PDF, JPG à¸«à¸£à¸·à¸­ PNG à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™")

@app.post("/analyze")
async def analyze_resume(resume_file: UploadFile = File(...)):
    global resume_uploaded, resume_cache, conversation_history

    base64_image = file_to_base64(resume_file)
    resume_uploaded = True

    conversation_history.append({
        "role": "system",
        "content": f"à¸™à¸µà¹ˆà¸„à¸·à¸­à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰:\n{resume_cache[:1000]}"
    })

    prompt = f"""
à¸„à¸¸à¸“à¸„à¸·à¸­ AI Resume Analyzer à¸Šà¹ˆà¸§à¸¢à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰:

ðŸ“ **à¸„à¸³à¸ªà¸±à¹ˆà¸‡:**
- à¸•à¸§à¸£à¸ˆà¸à¹ˆà¸­à¸™à¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¸¡à¸²à¹€à¸›à¹‡à¸™à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ à¸«à¸²à¸à¹„à¸¡à¹ˆà¸—à¸³à¸à¸²à¸£à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™ à¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆ à¹à¸¥à¸°à¸ªà¸´à¹‰à¸™à¸ªà¸¸à¸”à¸—à¸±à¸™à¸—à¸µà¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£à¸•à¹ˆà¸­à¸•à¸²à¸¡ propmt à¸•à¹ˆà¸­à¹„à¸›
- à¸Šà¸µà¹‰à¸ˆà¸¸à¸”à¹€à¸”à¹ˆà¸™à¹à¸¥à¸°à¸—à¸±à¸à¸©à¸°à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¸ªà¸¡à¸±à¸„à¸£
- à¸ªà¸à¸´à¸¥/à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–/à¸ªà¸à¸´à¸¥ à¸ªà¸£à¸¸à¸›à¸­à¸­à¸à¸¡à¸²à¹ƒà¸«à¹‰à¸”à¸¹ à¸žà¸£à¹‰à¸­à¸¡à¸„à¸°à¹à¸™à¸™ à¸«à¸²à¸à¸¡à¸µà¸£à¸°à¸šà¸¸à¹„à¸§à¹‰ (à¸ à¸²à¸¢à¹ƒà¸™à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸­à¸²à¸ˆà¸ˆà¸°à¸¡à¸µ bar chart à¸«à¸£à¸·à¸­ infographic à¸•à¹ˆà¸²à¸‡à¹†à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¸à¸©à¸° )
- à¸Šà¸µà¹‰à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¸„à¸§à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™
- à¹ƒà¸«à¹‰à¸„à¸°à¹à¸™à¸™à¹à¸•à¹ˆà¸¥à¸°à¸ªà¹ˆà¸§à¸™ (à¸—à¸±à¸à¸©à¸°/à¸›à¸£à¸°à¸ªà¸šà¸à¸²à¸£à¸“à¹Œ/à¸„à¸§à¸²à¸¡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ) à¸žà¸£à¹‰à¸­à¸¡ Emoji à¸™à¸³à¸«à¸™à¹‰à¸²à¸›à¸£à¸°à¹‚à¸¢à¸„ à¸”à¸±à¸‡à¸™à¸µà¹‰:
  - âœ… à¸„à¸£à¸šà¸–à¹‰à¸§à¸™ à¸•à¸£à¸‡à¸•à¸²à¸¡à¸‚à¹‰à¸­à¸à¸³à¸«à¸™à¸”
  - âŒ à¹„à¸¡à¹ˆà¸„à¸£à¸šà¸«à¸£à¸·à¸­à¸‚à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸„à¸±à¸
  - âš ï¸ à¹„à¸¡à¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™à¸«à¸£à¸·à¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¹€à¸žà¸µà¸¢à¸‡à¸žà¸­
- à¹ƒà¸«à¹‰à¸„à¸°à¹à¸™à¸™ **à¸„à¸§à¸²à¸¡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¹‚à¸”à¸¢à¸£à¸§à¸¡** à¸‚à¸­à¸‡à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¹€à¸›à¹‡à¸™ %
- à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢ à¹ƒà¸Šà¹‰ bullet list à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¹à¸¥à¸°à¹à¸šà¹ˆà¸‡à¸«à¸±à¸§à¸‚à¹‰à¸­à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¸«à¸™à¸¶à¹ˆà¸‡à¸«à¸±à¸§à¸‚à¹‰à¸­à¸«à¸™à¸¶à¹ˆà¸‡à¸šà¸£à¸£à¸—à¸±à¸”
- à¸—à¸¸à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ **à¸ à¸²à¸©à¸²à¹„à¸—à¸¢** à¹à¸¥à¸°à¸ªà¸²à¸¡à¸²à¸£à¸–à¸¡à¸µ **à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¸ªà¸±à¹‰à¸™à¹†** à¹„à¸”à¹‰ à¸«à¸²à¸à¸ˆà¸³à¹€à¸›à¹‡à¸™ à¸«à¹‰à¸²à¸¡à¸¡à¸µà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸ à¸²à¸©à¸²à¸­à¸·à¹ˆà¸™à¸­à¸­à¸à¸¡à¸²à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”

â„¹ï¸ à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¸«à¸²à¸à¸–à¸¹à¸à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰à¸£à¹ˆà¸§à¸¡à¸à¸±à¸šà¹‚à¸«à¸¡à¸”à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ à¹ƒà¸«à¹‰à¸„à¸¸à¸“à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸šà¸£à¸´à¸šà¸—à¸ªà¸³à¸«à¸£à¸±à¸šà¸–à¸²à¸¡-à¸•à¸­à¸šà¹„à¸”à¹‰à¸”à¹‰à¸§à¸¢ 
    à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆ à¹à¸¥à¸°à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¹‚à¸«à¸¡à¸”à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¸‚à¸­à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸œà¸¹à¹‰à¸ªà¸¡à¸±à¸„à¸£ à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸‚à¹‰à¸²à¸¡à¹„à¸›à¹€à¸¥à¸¢
"""

    answer = client.chat.completions.create(
        model="meta-llama/llama-4-scout-17b-16e-instruct",
        messages=[
            {"role": "system", "content": "à¸„à¸¸à¸“à¸„à¸·à¸­ HR AI Analyzer"},
            {"role": "user", "content": 
             [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },   
             ]
            },
        ],
        temperature=0.1,
        max_tokens=800,
    )

    report = answer.choices[0].message.content
    scores = extract_scores(report)
    avg_score = round(sum(scores) / len(scores), 2) if scores else None

    conversation_history.append({"role": "assistant", "content": report})
    resume_cache = report

    return JSONResponse({
        "report": report,
        "scores": scores,
        "average_score": avg_score
    })
class ChatRequest(BaseModel):
    message: str


@app.post("/chat")
async def chat_with_ai(req: ChatRequest):
    global conversation_history, resume_uploaded, resume_cache

    user_message = req.message
    if resume_uploaded:
        user_content = f"{user_message}\n\n(ðŸ“„ à¸šà¸£à¸´à¸šà¸—à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡: à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸„à¸·à¸­ {resume_cache}...)"
    else:
        user_content = user_message

    conversation_history.append({"role": "user", "content": user_content})

    try:
        prompt = f"""
        - à¸„à¸¸à¸“à¸„à¸·à¸­ Job Assistant à¸‚à¸­à¸‡à¹€à¸§à¹‡à¸šà¹„à¸‹à¸•à¹Œ Job.Scraper TH à¹€à¸§à¹‡à¸šà¹„à¸‹à¸•à¹Œà¸—à¸µà¹ˆà¸£à¸§à¸šà¸£à¸§à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‡à¸²à¸™à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢à¸ˆà¸²à¸à¹€à¸§à¹‡à¸š JobBKK JobTH JobThai [jobbkk.com, jobth.com, jobthai.com]
        - à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸•à¹ˆà¸­à¸‡à¸—à¸³à¸„à¸·à¸­à¸•à¸­à¸šà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£ à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢ à¹à¸¥à¸°à¹ƒà¸Šà¹‰ Emoji à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢à¹€à¸žà¸·à¹ˆà¸­à¹€à¸™à¹‰à¸™à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸ªà¸³à¸„à¸±à¸ à¹‚à¸”à¸¢à¹ƒà¸™à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¹€à¸£à¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸œà¸¹à¹‰à¸«à¸à¸´à¸‡ à¸¥à¸‡à¸—à¹‰à¸²à¸¢à¸„à¸³à¸”à¹‰à¸§à¸¢(à¸„à¸°/à¸„à¹ˆà¸°)à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
        - à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ªà¸§à¸±à¸ªà¸”à¸µà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡ à¸«à¸²à¸à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸šà¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆ à¹ƒà¸«à¹‰à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¹‚à¸¢à¸‡à¸šà¸£à¸´à¸šà¸—à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¹„à¸”à¹‰à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”
        - à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢ à¹ƒà¸Šà¹‰ bullet list à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¹à¸¥à¸°à¹à¸šà¹ˆà¸‡à¸«à¸±à¸§à¸‚à¹‰à¸­à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¸«à¸™à¸¶à¹ˆà¸‡à¸«à¸±à¸§à¸‚à¹‰à¸­à¸«à¸™à¸¶à¹ˆà¸‡à¸šà¸£à¸£à¸—à¸±à¸”
        - (à¸­à¸˜à¸´à¸šà¸²à¸¢à¹ƒà¸«à¹‰à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸Ÿà¸±à¸‡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸„à¸¸à¸“à¸«à¹‰à¸²à¸¡à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸à¸²à¸£à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œà¹ƒà¸”à¹†à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”) à¸ªà¸²à¸¡à¸²à¸£à¸–à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸«à¸£à¸·à¸­à¸ˆà¸³à¸¥à¸­à¸‡ **à¹‚à¸«à¸¡à¸”à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œà¸‡à¸²à¸™** à¹„à¸”à¹‰ à¹‚à¸”à¸¢à¸à¸²à¸£à¸à¸”à¸›à¸¸à¹ˆà¸¡"à¸ªà¸±à¸¡à¸ à¸²à¸©à¸“à¹Œ"à¸”à¹‰à¸²à¸™à¸‚à¸§à¸²à¸¥à¹ˆà¸²à¸‡à¹€à¸žà¸·à¹ˆà¸­à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹‚à¸«à¸¡à¸” à¸«à¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸–à¸²à¸¡à¹ƒà¸™à¸¥à¸±à¸à¸©à¸“à¸°à¸™à¸±à¹‰à¸™ à¹€à¸Šà¹ˆà¸™ à¸•à¸±à¹‰à¸‡à¸„à¸³à¸–à¸²à¸¡à¹à¸šà¸š HR 
        - à¸—à¸¸à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ **à¸ à¸²à¸©à¸²à¹„à¸—à¸¢** à¹à¸¥à¸°à¸ªà¸²à¸¡à¸²à¸£à¸–à¸¡à¸µ **à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¸ªà¸±à¹‰à¸™à¹†** à¸«à¸²à¸à¸ˆà¸³à¹€à¸›à¹‡à¸™ à¸«à¹‰à¸²à¸¡à¸¡à¸µà¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¸ à¸²à¸©à¸²à¸­à¸·à¹ˆà¸™à¸­à¸­à¸à¸¡à¸²à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”

à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸–à¸²à¸¡:
{user_message}

ðŸ“„ à¸šà¸£à¸´à¸šà¸—:
{"à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸¥à¹ˆà¸²à¸ªà¸¸à¸”: " + resume_cache[:500] + "..." if resume_uploaded else "à¹„à¸¡à¹ˆà¸¡à¸µà¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆ"}
"""
        answer = client.chat.completions.create(
            model="openai/gpt-oss-120b",
            messages=[
                {"role": "system", "content": "à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢ AI à¹à¸Šà¸—à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£ à¹ƒà¸Šà¹‰à¸­à¸µà¹‚à¸¡à¸ˆà¸´à¹à¸¥à¸°à¸ˆà¸±à¸” format à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
            max_tokens=500,
        )
        response_text = answer.choices[0].message.content
    except Exception as e:
        response_text = f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”: {str(e)}"

    conversation_history.append({"role": "assistant", "content": response_text})

    return JSONResponse({
        "reply": response_text,
        "history": conversation_history[-5:]
    })

@app.post("/recommend/cv")
async def analyze_resume(resume_file: UploadFile = File(...)):

    resume_text = extract_text(resume_file)
    conversation_history.append({
        "role": "system",
        "content": f"à¸™à¸µà¹ˆà¸„à¸·à¸­à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰:\n{resume_cache}"
    })

    prompt = f"""
à¸„à¸¸à¸“à¸„à¸·à¸­ AI Resume Analyzer à¸Šà¹ˆà¸§à¸¢à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰:

ðŸ“„ **à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸œà¸¹à¹‰à¸ªà¸¡à¸±à¸„à¸£:**
{resume_text}

ðŸ“ **à¸„à¸³à¸ªà¸±à¹ˆà¸‡:**
- à¹à¸™à¸°à¸™à¸³à¸‡à¸²à¸™à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸œà¸¹à¹‰à¸ªà¸¡à¸±à¸„à¸£à¸•à¸²à¸¡à¹€à¸£à¸‹à¸¹à¹€à¸¡à¹ˆà¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸¡à¸² à¸ˆà¸³à¸™à¸§à¸™ 5 à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¸‡à¸²à¸™
- à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸£à¸¹à¸›à¹à¸šà¸šà¸”à¸±à¸‡à¸™à¸µà¹‰: Job1,Job2,Job3,Job4,Job5
- à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡à¸„à¸³à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰ Junior, Senior, Intern
- text à¸„à¸§à¸£à¸¡à¸µà¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 2 à¸„à¸³ 
"""

    answer = client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[
            {"role": "system", "content": "à¸„à¸¸à¸“à¸„à¸·à¸­ HR AI Analyzer"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,
        max_tokens=1500,
    )

    response = answer.choices[0].message.content

    jobs = response.split(',')
    if len(jobs) >= 5:
        response = ','.join(jobs[:5])

    return JSONResponse({
        "reply": response,
        "jobs": jobs,
    })